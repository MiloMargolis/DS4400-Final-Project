{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting Redevelopment Potential for Boston Parcels\n",
        "\n",
        "**Authors:** Milo Margolis, Dhruv Rokkam\n",
        "\n",
        "**Problem Statement:** This project develops a binary classification model to predict the redevelopment potential for Boston properties using parcel data. Properties are labels as high potential based on indicators such as low building to land ratios and underutilized FAR and then classified using logistic regression, KNN, and decision trees with the proper training, validation, and hyperparameter tuning to demonstrate overfitting prevention. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 1: Importing Libraries and set random set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                            f1_score, confusion_matrix, roc_curve, auc)\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# set the random seed \n",
        "np.random.seed(42)\n",
        "\n",
        "# set the plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 2: Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Shape:\n",
            "(182393, 24)\n",
            "\n",
            "==================================================\n",
            "Dataset Info:\n",
            "==================================================\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 182393 entries, 0 to 182392\n",
            "Data columns (total 24 columns):\n",
            " #   Column                        Non-Null Count   Dtype  \n",
            "---  ------                        --------------   -----  \n",
            " 0   property_id                   182393 non-null  object \n",
            " 1   address                       182393 non-null  object \n",
            " 2   parcel_id                     182393 non-null  int64  \n",
            " 3   attributes                    182393 non-null  object \n",
            " 4   neighborhood                  182393 non-null  object \n",
            " 5   created_at                    182393 non-null  object \n",
            " 6   updated_at                    182393 non-null  object \n",
            " 7   actual_far                    148349 non-null  float64\n",
            " 8   building_value                182393 non-null  float64\n",
            " 9   classification_code           182393 non-null  int64  \n",
            " 10  far_utilization               127498 non-null  float64\n",
            " 11  fiscal_year                   182393 non-null  int64  \n",
            " 12  land_to_building_value_ratio  161951 non-null  float64\n",
            " 13  land_value                    182393 non-null  float64\n",
            " 14  living_area                   148323 non-null  float64\n",
            " 15  gross_area                    148639 non-null  float64\n",
            " 16  lot_size                      175022 non-null  float64\n",
            " 17  owner_address                 182388 non-null  object \n",
            " 18  owner_name                    182388 non-null  object \n",
            " 19  permits                       182393 non-null  object \n",
            " 20  total_assessed_value          182393 non-null  float64\n",
            " 21  vacant_lot                    182393 non-null  bool   \n",
            " 22  year_built                    159659 non-null  float64\n",
            " 23  zoning                        182393 non-null  object \n",
            "dtypes: bool(1), float64(10), int64(3), object(10)\n",
            "memory usage: 32.2+ MB\n"
          ]
        }
      ],
      "source": [
        "# Load CSV file\n",
        "data_path = Path(\"../data/raw/boston_properties.csv\")\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"Dataset Shape:\")\n",
        "print(df.shape)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Dataset Info:\")\n",
        "print(\"=\"*50)\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values:\n",
            "==================================================\n",
            "property_id                         0\n",
            "address                             0\n",
            "parcel_id                           0\n",
            "attributes                          0\n",
            "neighborhood                        0\n",
            "created_at                          0\n",
            "updated_at                          0\n",
            "actual_far                      34044\n",
            "building_value                      0\n",
            "classification_code                 0\n",
            "far_utilization                 54895\n",
            "fiscal_year                         0\n",
            "land_to_building_value_ratio    20442\n",
            "land_value                          0\n",
            "living_area                     34070\n",
            "gross_area                      33754\n",
            "lot_size                         7371\n",
            "owner_address                       5\n",
            "owner_name                          5\n",
            "permits                             0\n",
            "total_assessed_value                0\n",
            "vacant_lot                          0\n",
            "year_built                      22734\n",
            "zoning                              0\n",
            "dtype: int64\n",
            "\n",
            "==================================================\n",
            "Total missing values: 207320\n",
            "Columns with missing values: 9\n"
          ]
        }
      ],
      "source": [
        "# Check missing values\n",
        "print(\"Missing Values:\")\n",
        "print(\"=\"*50)\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "print(f\"Columns with missing values: {(missing_values > 0).sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "print(\"Handling missing values...\")\n",
        "print(f\"Initial shape: {df.shape}\")\n",
        "\n",
        "# Decision 1: Drop rows with missing owner_address or owner_name (only 5 rows)\n",
        "# Reason: Very few missing values (0.003% of data), and these are categorical text fields\n",
        "# that cannot be reliably imputed. Dropping is cleaner than imputing placeholder text.\n",
        "df = df.dropna(subset=['owner_address', 'owner_name'])\n",
        "print(f\"After dropping rows with missing owner info: {df.shape}\")\n",
        "\n",
        "# Decision 2: Drop rows missing far_utilization or actual_far\n",
        "# Reason: These columns are critical for creating our target variable (high_potential).\n",
        "# We cannot reliably impute these values as they are key features for redevelopment potential.\n",
        "# This removes ~30% of data but ensures data quality for modeling.\n",
        "df = df.dropna(subset=['far_utilization', 'actual_far'])\n",
        "print(f\"After dropping rows with missing FAR data: {df.shape}\")\n",
        "\n",
        "# Decision 3: Calculate land_to_building_value_ratio where possible\n",
        "# Reason: This ratio can be calculated from land_value and building_value when missing.\n",
        "# Only calculate if both source values are present and ratio is missing.\n",
        "mask_missing_ratio = df['land_to_building_value_ratio'].isna()\n",
        "mask_has_values = (df['land_value'] > 0) & (df['building_value'] > 0)\n",
        "calculated_count = (mask_missing_ratio & mask_has_values).sum()\n",
        "df.loc[mask_missing_ratio & mask_has_values, 'land_to_building_value_ratio'] = \\\n",
        "    df.loc[mask_missing_ratio & mask_has_values, 'land_value'] / \\\n",
        "    df.loc[mask_missing_ratio & mask_has_values, 'building_value']\n",
        "print(f\"Calculated {calculated_count} missing ratios where possible\")\n",
        "\n",
        "# Decision 4: Impute remaining numeric columns with median\n",
        "# Reason: Median is robust to outliers and appropriate for continuous variables.\n",
        "# We impute: living_area, gross_area, year_built, lot_size, land_to_building_value_ratio\n",
        "numeric_cols_to_impute = ['living_area', 'gross_area', 'year_built', 'lot_size', \n",
        "                          'land_to_building_value_ratio']\n",
        "for col in numeric_cols_to_impute:\n",
        "    if df[col].isna().sum() > 0:\n",
        "        median_value = df[col].median()\n",
        "        df[col].fillna(median_value, inplace=True)\n",
        "        print(f\"Imputed {col} with median: {median_value:.2f}\")\n",
        "\n",
        "print(f\"\\nFinal shape after handling missing values: {df.shape}\")\n",
        "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Missing Values Summary:**\n",
        "- Dropped 5 rows with missing owner information (negligible impact, <0.003% of data).\n",
        "- Dropped rows missing `far_utilization` or `actual_far` (~30% of data) as these are critical for target creation and cannot be reliably imputed.\n",
        "- Calculated `land_to_building_value_ratio` from source values where possible.\n",
        "- Imputed remaining numeric columns (`living_area`, `gross_area`, `year_built`, `lot_size`, `land_to_building_value_ratio`) with median values, which is robust to outliers for continuous variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check and remove duplicate rows\n",
        "print(\"Checking for duplicate rows...\")\n",
        "print(f\"Shape before removing duplicates: {df.shape}\")\n",
        "\n",
        "# Check for exact duplicate rows (all columns match)\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
        "\n",
        "# Decision: Remove duplicate rows if any exist\n",
        "# Reason: Duplicate rows provide no additional information and can bias the model\n",
        "# by giving more weight to certain property records. We keep the first occurrence.\n",
        "if duplicate_count > 0:\n",
        "    df = df.drop_duplicates()\n",
        "    print(f\"Removed {duplicate_count} duplicate rows\")\n",
        "else:\n",
        "    print(\"No duplicate rows found\")\n",
        "\n",
        "print(f\"Shape after removing duplicates: {df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Duplicate Removal Summary:**\n",
        "- Checked for exact duplicate rows across all columns.\n",
        "- Removed duplicate rows (if any) to prevent data bias in modeling, keeping only the first occurrence of each unique row.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
